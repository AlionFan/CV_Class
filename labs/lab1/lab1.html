<div id="lab1">
    <h1>ç¬¬ä¸€å‘¨å®éªŒ</h1>
    <h2>1. ä¿®æ”¹åŒºåŸŸé¢œè‰²</h2>
    
    <div class="image-grid-container">
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task1/modified_lenacolor.png" alt="ä¿®æ”¹åŒºåŸŸé¢œè‰²">
        </div>
    </div>

    <pre><code class="language-python">
import cv2
import numpy as np
img = cv2.imread('jupyter/lenacolor.png')
img[0,0, 0:3]
img1 = img.copy()
img1[0:50, 0:100, :] = [255, 255, 255]
img1[50:100, 0:100, :] = [128, 128, 128]
img1[100:150, 0:100, :] = [0, 0, 0]
img1[150:200, 0:100, :] = [0, 0, 255]
display(img1[0, 0, 0:3])
cv2.imwrite('result/task1/modified_lenacolor.png', img1)
    </code></pre>

    <h2>2. è¯»å–å½©è‰²å›¾ç‰‡å¹¶åˆ›å»ºæ©ç </h2>
    <div class="image-grid-container">
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task2/gray_x.jpg" alt="ç°åº¦å›¾åƒ">
        </div>
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task2/mask.png" alt="æ©ç å›¾åƒ">
        </div>
    </div>

    <pre><code class="language-python">
img = cv2.imread('jupyter/x.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
# mask = np.zeros(img.shape, dtype=np.uint8)
mask = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)[1]
cv2.imwrite("result/task2/mask.png", mask)
cv2.imwrite('result/task2/gray_x.jpg', img)
    </code></pre>
    
    <h2>3.å½¢æ€å­¦æ“ä½œ</h2>
    <div class="image-grid-container">
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task3/open_4.jpg" alt="å¼€è¿ç®—">
            <p class="image-grid-title">å¼€è¿ç®—</p>
        </div>
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task3/close_6.jpg" alt="é—­è¿ç®—">
            <p class="image-grid-title">é—­è¿ç®—</p>
        </div>

        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task3/tophat_4.jpg" alt="é¡¶å¸½æ“ä½œ">
            <p class="image-grid-title">é¡¶å¸½æ“ä½œ</p>
        </div>
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task3/blackhat_6.jpg" alt="é»‘å¸½æ“ä½œ">
            <p class="image-grid-title">é»‘å¸½æ“ä½œ</p>
        </div>

        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task3/gradient_5.jpg" alt="æ¢¯åº¦è¿ç®—">
            <p class="image-grid-title">æ¢¯åº¦è¿ç®—</p>
        </div>
    </div>

    <pre><code class="language-python">
img = cv2.imread("jupyter/4.jpg")

img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)
cv2.imwrite("result/task3/open_4.jpg", opening)

tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)
cv2.imwrite("result/task3/tophat_4.jpg", tophat)

img = cv2.imread("jupyter/5.jpg")
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)
cv2.imwrite("result/task3/gradient_5.jpg",gradient)

img = cv2.imread("jupyter/6.jpg")
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
close = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)
cv2.imwrite("result/task3/close_6.jpg", close)

blackhat = cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, kernel)
cv2.imwrite("result/task3/blackhat_6.jpg", blackhat)
    </code></pre>
        
    <h2>4.å›¾åƒé€è§†å˜æ¢ä¸ OCR æ–‡å­—è¯†åˆ«</h2>
    <div class="image-grid-container">
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task4/orig.jpg" alt="åŸå›¾">
        </div>
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task4/edged.jpg" alt="è¾¹ç¼˜æ£€æµ‹">
        </div>
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task4/outline.jpg" alt="æ¡†é€‰ç›®æ ‡">
        </div>
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task4/scan.jpg" alt="é€è§†å˜æ¢ç»“æœ">
        </div>
    </div>

    <div class="receipt-content">
        <p>WHOLE FOODS MARKET</p>
        <p>399 POST RD WEST</p>
        <p>WESTPORT, CT 06880</p>
        <p>(203) 227-6858</p>
        <br>
        <p>BACON</p>
        <p>CHICKEN BREAST BONELESS SKINLESS</p>
        <p>HEAVY CREAM</p>
        <p>BALSAMIC REDUCTION</p>
        <p>BEEF GROUND 85/15</p>
        <p>JUICE COFFEE CASHEW</p>
        <p>PINT ORGANIC</p>
        <p>HONEY ALMOND BUTTER</p>
        <p>FLOUR ALMOND</p>
        <br>
        <p>SUBTOTAL: $101.33</p>
        <p>TAX: $4.99</p>
        <p>TOTAL: $106.32</p>
    </div>

    <p>è¯†åˆ«çš„ä¸æ˜¯å¾ˆå‡†å•Šï¼Œæ€»é‡‘é¢ğŸ’°éƒ½æ˜¯é”™çš„,å‰é¢ä¸æ˜¯å¾ˆé‡è¦çš„æ–‡å­—è¯†åˆ«å€’æ˜¯æŒºå‡†çš„ã€‚</p>
    <pre><code class="language-python">
import numpy as np
import argparse
import cv2
import os
import pytesseract
from PIL import Image

def order_points(pts):
    # ä¸€å…±4ä¸ªåæ ‡ç‚¹
    rect = np.zeros((4, 2), dtype = "float32")

    # æŒ‰é¡ºåºæ‰¾åˆ°å¯¹åº”åæ ‡0123åˆ†åˆ«æ˜¯ å·¦ä¸Šï¼Œå³ä¸Šï¼Œå³ä¸‹ï¼Œå·¦ä¸‹
    # è®¡ç®—å·¦ä¸Šï¼Œå³ä¸‹
    s = pts.sum(axis = 1)
    rect[0] = pts[np.argmin(s)]
    rect[2] = pts[np.argmax(s)]

    # è®¡ç®—å³ä¸Šå’Œå·¦ä¸‹
    diff = np.diff(pts, axis = 1)
    rect[1] = pts[np.argmin(diff)]
    rect[3] = pts[np.argmax(diff)]

    return rect

def four_point_transform(image, pts):
    # è·å–è¾“å…¥åæ ‡ç‚¹
    rect = order_points(pts)
    (tl, tr, br, bl) = rect

    # è®¡ç®—è¾“å…¥çš„wå’Œhå€¼
    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))
    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))
    maxWidth = max(int(widthA), int(widthB))

    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))
    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))
    maxHeight = max(int(heightA), int(heightB))

    # å˜æ¢åå¯¹åº”åæ ‡ä½ç½®
    dst = np.array([
        [0, 0],
        [maxWidth - 1, 0],
        [maxWidth - 1, maxHeight - 1],
        [0, maxHeight - 1]], dtype = "float32")

    # è®¡ç®—å˜æ¢çŸ©é˜µ
    M = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))

    # è¿”å›å˜æ¢åç»“æœ
    return warped

def resize(image, width=None, height=None, inter=cv2.INTER_AREA):
    dim = None
    (h, w) = image.shape[:2]
    if width is None and height is None:
        return image
    if width is None:
        r = height / float(h)
        dim = (int(w * r), height)
    else:
        r = width / float(w)
        dim = (width, int(h * r))
    resized = cv2.resize(image, dim, interpolation=inter)
    return resized

# è¯»å–è¾“å…¥
image = cv2.imread("jupyter/receipt.jpg")
#åæ ‡ä¹Ÿä¼šç›¸åŒå˜åŒ–
ratio = image.shape[0] / 500.0
orig = image.copy()


image = resize(orig, height = 500)

# é¢„å¤„ç†
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (5, 5), 0)
edged = cv2.Canny(gray, 75, 200)

# å±•ç¤ºé¢„å¤„ç†ç»“æœ
print("STEP 1: è¾¹ç¼˜æ£€æµ‹")
cv2.imshow("Image", image)
cv2.imshow("Edged", edged)
cv2.imwrite("result/task4/edged.jpg", edged)
cv2.waitKey(0)
cv2.destroyAllWindows()

# è½®å»“æ£€æµ‹
cnts, _ = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]

# éå†è½®å»“
for c in cnts:
    # è®¡ç®—è½®å»“è¿‘ä¼¼
    peri = cv2.arcLength(c, True)
    # Cè¡¨ç¤ºè¾“å…¥çš„ç‚¹é›†
    # epsilonè¡¨ç¤ºä»åŸå§‹è½®å»“åˆ°è¿‘ä¼¼è½®å»“çš„æœ€å¤§è·ç¦»ï¼Œå®ƒæ˜¯ä¸€ä¸ªå‡†ç¡®åº¦å‚æ•°
    # Trueè¡¨ç¤ºå°é—­çš„
    approx = cv2.approxPolyDP(c, 0.02 * peri, True)

    # 4ä¸ªç‚¹çš„æ—¶å€™å°±æ‹¿å‡ºæ¥
    if len(approx) == 4:
        screenCnt = approx
        break

# å±•ç¤ºç»“æœ
print("STEP 2: è·å–è½®å»“")
cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)
cv2.imshow("Outline", image)
cv2.imwrite("result/task4/outline.jpg", image)
cv2.waitKey(0)
cv2.destroyAllWindows()

# é€è§†å˜æ¢
warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio)

# äºŒå€¼å¤„ç†
warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)
ref = cv2.threshold(warped, 100, 255, cv2.THRESH_BINARY)[1]
cv2.imwrite('result/task4/scan.jpg', ref)

# å±•ç¤ºç»“æœ
print("STEP 3: å˜æ¢")
cv2.imshow("Original", resize(orig, height = 650))
cv2.imshow("Scanned", resize(ref, height = 650))
cv2.imwrite("result/task4/orig.jpg", orig)
cv2.imwrite("result/task4/ref.jpg", ref)
cv2.waitKey(0)

# æ–‡å­—æå–
print("STEP 4: æ–‡å­—æå–")
# å¯¹å›¾åƒè¿›è¡Œé¢å¤–çš„é¢„å¤„ç†ä»¥æé«˜æ–‡å­—è¯†åˆ«æ•ˆæœ
kernel = np.ones((2,2), np.uint8)
ref = cv2.dilate(ref, kernel, iterations=1)
ref = cv2.erode(ref, kernel, iterations=1)

# å°†OpenCVå›¾åƒè½¬æ¢ä¸ºPILå›¾åƒ
pil_image = Image.fromarray(ref)

# ä½¿ç”¨pytesseractæå–æ–‡å­—
text = pytesseract.image_to_string(pil_image, lang='chi_sim+eng')

# å°†æå–çš„æ–‡å­—ä¿å­˜åˆ°æ–‡ä»¶
with open('result/task4/extracted_text.txt', 'w', encoding='utf-8') as f:
    f.write(text)

print("æå–çš„æ–‡å­—å†…å®¹ï¼š")
print(text)
print("\næ–‡å­—å·²ä¿å­˜åˆ° result/task4/extracted_text.txt") 
    </code></pre>


    <h2>5.å¤šäººè„¸æ£€æµ‹</h2>
    <div class="image-grid-container">
        
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task5/face_result.jpg" alt="æ£€æµ‹ç»“æœ">
        </div>
    </div>
    <p>æœ‰ä¸¤ä¸ªäºº(2, 6)æŠŠå˜´å·´ğŸ‘„è¯†åˆ«æˆçœ¼ç›äº†,(4)çš„å¢™è¯†åˆ«æˆäººè„¸äº†ï¼Œæ€ªå“äººçš„ã€‚</p>

    <pre><code class="language-python">
import cv2

image = cv2.imread('jupyter/manyPeople.jpg') 
# å°†å½©è‰²å›¾åƒè½¬æ¢ä¸ºç°åº¦å›¾åƒ
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# åŠ è½½é¢éƒ¨æ£€æµ‹åˆ†ç±»å™¨
faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 
'haarcascade_frontalface_default.xml')  # åŠ è½½é¢„è®­ç»ƒçš„äººè„¸æ£€æµ‹åˆ†ç±»å™¨

# åŠ è½½çœ¼ç›æ£€æµ‹åˆ†ç±»å™¨
eyeCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
# åŠ è½½é¢„è®­ç»ƒçš„çœ¼ç›æ£€æµ‹åˆ†ç±»å™¨

# æ£€æµ‹ç°åº¦å›¾åƒä¸­çš„äººè„¸
faces = faceCascade.detectMultiScale(gray, 
scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

print("å‘ç°{0}å¼ äººè„¸!".format(len(faces)))  # æ‰“å°æ£€æµ‹åˆ°çš„äººè„¸æ•°é‡
print("å…¶ä½ç½®åˆ†åˆ«æ˜¯ï¼š")
print(faces)  # æ‰“å°äººè„¸çš„åæ ‡å’Œå¤§å°ä¿¡æ¯ï¼ˆx, y, w, hï¼‰

for (x, y, w, h) in faces:  # éå†æ£€æµ‹åˆ°çš„æ¯å¼ äººè„¸
    # åœ¨å›¾åƒä¸Šç»˜åˆ¶ç»¿è‰²çš„äººè„¸çŸ©å½¢æ¡†
    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)


    # æ£€æµ‹çœ¼ç›åŒºåŸŸ
    eyes = eyeCascade.detectMultiScale(gray, 
    scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    # åœ¨äººè„¸åŒºåŸŸç»˜åˆ¶è“è‰²çš„çœ¼ç›çŸ©å½¢æ¡†
    for (ex, ey, ew, eh) in eyes:
        cv2.rectangle(image, (ex, ey), (ex + ew, ey + eh), (0, 0, 255), 2)

# ä¿å­˜å¸¦æœ‰äººè„¸å’Œçœ¼ç›æ£€æµ‹æ¡†çš„å›¾åƒ
cv2.imwrite("result/task5/face_result.jpg", image)
    </code></pre>

    <div class="receipt-content">
        <p>å‘ç°11å¼ äººè„¸!</p>
        <p>å…¶ä½ç½®åˆ†åˆ«æ˜¯ï¼š</p>
        <p>[[  97   99  169  169]</p>
        <p>[1147  104  160  160]</p>
        <p>[ 843  200   70   70]</p>
        <p>[ 888  104  167  167]</p>
        <p>[ 361   96  166  166]</p>
        <p>[ 623  101  162  162]</p>
        <p>[  99  375  160  160]</p>
        <p>[ 884  373  161  161]</p>
        <p>[1142  369  167  167]</p>
        <p>[ 357  373  165  165]</p>
        <p>[ 626  383  151  151]</p>
        <p>åä¸ªäºº åä¸€å¼ è„¸ï¼Œå¢™é¢ç®—ä¸€ä¸ªã€‚</p>
    </div>


    <h2>6.åŸºäº MeanShift çš„ç›®æ ‡è·Ÿè¸ª && 7. æ”¹è¿›ç®—æ³•</h2>

    <video width="600" controls autoplay muted preload="none">
        <source src="{{ '/labs/lab1/pic/task6/video.mp4' | relative_url }}" type="video/mp4">
    </video>
    <video width="600" controls autoplay muted preload="none">
        <source src="{{ '/labs/lab1/pic/task7/video.mp4' | relative_url }}" type="video/mp4">
    </video>

    <pre><code class="language-python">
import cv2 as cv 
import numpy as np

# æ‰“å¼€è§†é¢‘æ–‡ä»¶
cap = cv.VideoCapture("jupyter/video.mp4")

# è®¾ç½®åˆå§‹è·Ÿè¸ªçª—å£ (x, y, w, h)
track_window = (800, 400, 200, 160)  
ret, frame = cap.read()  
# è¯»å–è§†é¢‘çš„ç¬¬ä¸€å¸§ï¼Œret è¡¨ç¤ºæ˜¯å¦æˆåŠŸï¼Œframe æ˜¯å›¾åƒæ•°æ®
x, y, w, h = track_window  

# æå–åˆå§‹åŒºåŸŸå¹¶è½¬æ¢ä¸º HSV é¢œè‰²ç©ºé—´
hsv_roi = cv.cvtColor(frame, cv.COLOR_BGR2HSV)

mask = cv.inRange(hsv_roi, np.array((0., 60., 32.)), np.array((180., 255., 255.)))
# åˆ›å»ºæ©ç ï¼Œè¿‡æ»¤ç‰¹å®šé¢œè‰²èŒƒå›´

# è®¡ç®—ç›®æ ‡åŒºåŸŸçš„è‰²è°ƒç›´æ–¹å›¾å¹¶å½’ä¸€åŒ–
roi_hist = cv.calcHist([hsv_roi], [0], mask, [180], [0, 180])
cv.normalize(roi_hist, roi_hist, 0, 255, cv.NORM_MINMAX)

# è®¾ç½® MeanShift ç»ˆæ­¢æ¡ä»¶ä¸ºæœ€å¤§ 10 æ¬¡è¿­ä»£æˆ–çª—å£ç§»åŠ¨å°äº 1 åƒç´ 
termcriteria = (cv.TERM_CRITERIA_COUNT | cv.TERM_CRITERIA_EPS, 10, 1)


# åˆå§‹åŒ–è§†é¢‘å†™å…¥å¯¹è±¡
fourcc = cv.VideoWriter_fourcc(*'mp4v')  # å®šä¹‰è§†é¢‘ç¼–ç æ ¼å¼ä¸º MP4V
out = cv.VideoWriter("result/task6/tracked_video.mp4",
    fourcc, 30.0, (frame.shape[1], frame.shape[0]))  
# åˆ›å»ºè¾“å‡ºè§†é¢‘ï¼Œå¸§ç‡ 30ï¼Œå°ºå¯¸ä¸è¾“å…¥å¸§ç›¸åŒ

# è§†é¢‘å¸§å¾ªç¯å¤„ç†
while True:
    ret, frame = cap.read()  # è¯»å–è§†é¢‘çš„ä¸‹ä¸€å¸§
    if ret:
        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)  # å°†å½“å‰å¸§è½¬æ¢ä¸º HSV é¢œè‰²ç©ºé—´
        # æ ¹æ®ç›®æ ‡ç›´æ–¹å›¾è®¡ç®—åæŠ•å½±å›¾åƒ
        dst = cv.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)

        # åº”ç”¨ MeanShift è·Ÿè¸ª
        ret, track_window = cv.meanShift(dst, track_window, termcriteria)
        
        x, y, w, h = track_window  # è·å–æ›´æ–°åçš„çª—å£åæ ‡å’Œå¤§å°
        img2 = cv.rectangle(frame, (x, y), (x+w, y+h), 255, 2)  
        # åœ¨å½“å‰å¸§ä¸Šç»˜åˆ¶ç™½è‰²è·Ÿè¸ªæ¡†ï¼Œçº¿å®½ä¸º 2
        out.write(img2)  # å°†å¸¦è·Ÿè¸ªæ¡†çš„å¸§å†™å…¥è¾“å‡ºè§†é¢‘
    else:
        break  # å¦‚æœå¸§è¯»å–å¤±è´¥ï¼ˆè§†é¢‘ç»“æŸï¼‰ï¼Œé€€å‡ºå¾ªç¯

# é‡Šæ”¾èµ„æº
cap.release() 
out.release()
    </code></pre>

    <p>æ”¹è¿›ç®—æ³•</p>
    <pre><code class="language-python">
import numpy as np
import dlib
import cv2

def main():
    # æ‰“å¼€è§†é¢‘æ–‡ä»¶
    vs = cv2.VideoCapture("jupyter/video.mp4")

    # è·å–è§†é¢‘çš„ç¬¬ä¸€å¸§
    ret, first_frame = vs.read()
    if ret:
        # è®¾ç½®åˆå§‹è¿½è¸ªåŒºåŸŸï¼ˆä½¿ç”¨åŸå§‹åæ ‡ï¼Œä¸è¿›è¡Œç¼©æ”¾ï¼‰
        x, y, w, h = 800, 400, 200, 160
        
        # è·å–åŸå§‹è§†é¢‘å°ºå¯¸
        (height, width) = first_frame.shape[:2]
        
        # åˆ›å»ºdlibè¿½è¸ªå™¨
        tracker = dlib.correlation_tracker()
        rect = dlib.rectangle(x, y, x + w, y + h)
        rgb = cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB)
        tracker.start_track(rgb, rect)
        
        # è®¾ç½®è¾“å‡ºè§†é¢‘ï¼ˆä½¿ç”¨åŸå§‹å°ºå¯¸ï¼‰
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter("result/task7/dlib_tracked.mp4", 
            fourcc, 30.0, (width, height))

        # è§†é¢‘æµ
        while True:
            # è¯»å–å½“å‰å¸§
            ret, frame = vs.read()
            if not ret:
                break
                
            # è½¬æ¢é¢œè‰²ç©ºé—´ç”¨äºè¿½è¸ª
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # æ›´æ–°è¿½è¸ªå™¨
            tracker.update(rgb)
            pos = tracker.get_position()
            
            # è·å–è¿½è¸ªæ¡†åæ ‡
            startX = int(pos.left())
            startY = int(pos.top())
            endX = int(pos.right())
            endY = int(pos.bottom())
            
            # ç»˜åˆ¶è¿½è¸ªæ¡†
            cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)
            cv2.putText(frame, "Tracking", (startX, startY - 10), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            # å†™å…¥è¾“å‡ºè§†é¢‘
            out.write(frame)
            
            # æŒ‰ESCé€€å‡º
            if cv2.waitKey(100) & 0xFF == 27:
                break

    # é‡Šæ”¾èµ„æº
    vs.release()
    out.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
    </code></pre>

    <h2>8.è½¦é“çº¿æ£€æµ‹</h2>
    <div class="image-grid-container">
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task8/resultline_img.jpg" alt="è½¦é“çº¿æ£€æµ‹">
        </div>
    </div>
    <pre><code class="language-python">
import numpy as np
import cv2 as cv 

# ç°åº¦å›¾è½¬æ¢
def grayscale(image):
    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
# Canny è¾¹ç¼˜æ£€æµ‹
def canny(image, low_threshold, high_threshold):
    return cv2.Canny(image, low_threshold, high_threshold)

# é«˜æ–¯æ»¤æ³¢
def gaussian_blur(image, kernel_size):
    return cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)

# ç”Ÿæˆå…´è¶£åŒºåŸŸå³ Mask æ©æ¨¡
def region_of_interest(image, vertices):
    mask = np.zeros_like(image)  # åˆ›å»ºä¸è¾“å…¥å›¾åƒå¤§å°ç›¸åŒçš„é›¶çŸ©é˜µï¼Œä½œä¸ºæ©æ¨¡
    if len(image.shape) > 2:  # å¦‚æœè¾“å…¥æ˜¯å½©è‰²å›¾åƒ
        ignore_mask_color = (255,) * image.shape[2]  # è®¾ç½®å¡«å……é¢œè‰²ä¸ºç™½è‰²ï¼ˆå¤šé€šé“ï¼‰
    
    else:  # å¦‚æœè¾“å…¥æ˜¯ç°åº¦å›¾åƒ
        ignore_mask_color = 255  # è®¾ç½®å¡«å……é¢œè‰²ä¸ºç™½è‰²ï¼ˆå•é€šé“ï¼‰
    
    # ç”¨ç™½è‰²å¡«å……å¤šè¾¹å½¢åŒºåŸŸï¼Œç”Ÿæˆæ©æ¨¡
    cv.fillPoly(mask, vertices, ignore_mask_color)  
    masked_image = cv.bitwise_and(image, mask)  # ä¸åŸå›¾åƒè¿›è¡Œä½ä¸è¿ç®—ï¼Œä¿ç•™æ„Ÿå…´è¶£åŒºåŸŸ
    return masked_image

# éœå¤«å˜æ¢æ£€æµ‹ç›´çº¿
def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):
    # rho: è·ç¦»ç²¾åº¦ï¼Œtheta: è§’åº¦ç²¾åº¦ï¼Œthreshold: ç´¯åŠ é˜ˆå€¼
    # min_line_len: æœ€å°çº¿æ®µé•¿åº¦ï¼Œmax_line_gap: æœ€å¤§æ–­è£‚é•¿åº¦
    lines = cv.HoughLinesP(img, rho, theta, 
        threshold, np.array([]), minLineLength=min_line_len, 
        maxLineGap=max_line_gap)
    return lines  # è¿”å›æ£€æµ‹åˆ°çš„çº¿æ®µï¼Œæ¯æ¡çº¿æ®µç”±èµ·ç‚¹å’Œç»ˆç‚¹åæ ‡è¡¨ç¤º

# ç»˜åˆ¶è½¦é“çº¿
def draw_lines(image, lines, color=[255,0,0], thickness=2):
    right_y_set, right_x_set, right_slope_set = [], [], []  
    # å­˜å‚¨å³è½¦é“çº¿çš„ y åæ ‡ã€x åæ ‡å’Œæ–œç‡
    left_y_set, left_x_set, left_slope_set = [], [], []  
    # å­˜å‚¨å·¦è½¦é“çº¿çš„ y åæ ‡ã€x åæ ‡å’Œæ–œç‡
    slope_min, slope_max = 0.35, 0.85  # å®šä¹‰è½¦é“çº¿æ–œç‡çš„æœ‰æ•ˆèŒƒå›´
    middle_x = image.shape[1] / 2  # è®¡ç®—å›¾åƒæ°´å¹³ä¸­çº¿ä½ç½®
    max_y = image.shape[0]  # è·å–å›¾åƒé«˜åº¦
    for line in lines:  # éå†æ‰€æœ‰æ£€æµ‹åˆ°çš„çº¿æ®µ
        for x1, y1, x2, y2 in line:  
        # æå–çº¿æ®µçš„èµ·ç‚¹ (x1, y1) å’Œç»ˆç‚¹ (x2, y2)
            fit = np.polyfit((x1, x2), (y1, y2), 1)  
            # å¯¹çº¿æ®µè¿›è¡Œä¸€æ¬¡å¤šé¡¹å¼æ‹Ÿåˆ
            slope = fit[0]  # è·å–æ–œç‡
            if slope_min < np.absolute(slope) <= slope_max:  
            # æ£€æŸ¥æ–œç‡æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                if slope > 0 and x1 > middle_x and x2 > middle_x:  
                # åˆ¤æ–­ä¸ºå³è½¦é“çº¿ï¼ˆæ­£æ–œç‡ä¸”åœ¨å›¾åƒå³ä¾§ï¼‰
                    right_y_set.extend([y1, y2])
                    right_x_set.extend([x1, x2])
                    right_slope_set.append(slope)
                elif slope < 0 and x1 < middle_x and x2 < middle_x:  
                # åˆ¤æ–­ä¸ºå·¦è½¦é“çº¿ï¼ˆè´Ÿæ–œç‡ä¸”åœ¨å›¾åƒå·¦ä¾§ï¼‰
                    left_y_set.extend([y1, y2])
                    left_x_set.extend([x1, x2])
                    left_slope_set.append(slope)
    # ç»˜åˆ¶å·¦è½¦é“çº¿
    if left_y_set:  # å¦‚æœæ£€æµ‹åˆ°å·¦è½¦é“çº¿æ•°æ®
        lindex = left_y_set.index(min(left_y_set))  
        # æ‰¾åˆ°æœ€é«˜ç‚¹ï¼ˆy åæ ‡æœ€å°ï¼‰çš„ç´¢å¼•
        left_x_top, left_y_top = left_x_set[lindex], left_y_set[lindex]  
        # è·å–é¡¶éƒ¨åæ ‡
        lslope = np.median(left_slope_set)  # ä½¿ç”¨æ–œç‡ä¸­å€¼æé«˜ç¨³å®šæ€§
        left_x_bottom = int(left_x_top + (max_y - left_y_top) / lslope)  
        # æ ¹æ®æ–œç‡è®¡ç®—åº•éƒ¨ x åæ ‡
        cv.line(image, (left_x_bottom, max_y), 
            (left_x_top, left_y_top), color, thickness)  
        # ç»˜åˆ¶å·¦è½¦é“çº¿
    # ç»˜åˆ¶å³è½¦é“çº¿
    if right_y_set:  # å¦‚æœæ£€æµ‹åˆ°å³è½¦é“çº¿æ•°æ®
        rindex = right_y_set.index(min(right_y_set))  
        # æ‰¾åˆ°æœ€é«˜ç‚¹ï¼ˆy åæ ‡æœ€å°ï¼‰çš„ç´¢å¼•
        right_x_top, right_y_top = right_x_set[rindex], right_y_set[rindex]  
        # è·å–é¡¶éƒ¨åæ ‡
        rslope = np.median(right_slope_set)  
        # ä½¿ç”¨æ–œç‡ä¸­å€¼æé«˜ç¨³å®šæ€§
        right_x_bottom = int(right_x_top + (max_y - right_y_top) / rslope)  
        # æ ¹æ®æ–œç‡è®¡ç®—åº•éƒ¨ x åæ ‡
        cv.line(image, (right_x_bottom, max_y), 
            (right_x_top, right_y_top), color, thickness)  
        # ç»˜åˆ¶å³è½¦é“çº¿
    return image  # è¿”å›ç»˜åˆ¶è½¦é“çº¿çš„å›¾åƒ

# å›¾åƒèåˆ
def weighted_img(img, initial_img, a=0.8, b=1., c=0.):
    return cv2.addWeighted(initial_img, a, img, b, c)
# ä¸»å¤„ç†å‡½æ•°
def process_image(image):
    rho = 1  # éœå¤«å˜æ¢çš„è·ç¦»ç²¾åº¦ï¼ˆåƒç´ ï¼‰
    theta = np.pi / 180  # éœå¤«å˜æ¢çš„è§’åº¦ç²¾åº¦ï¼ˆ1åº¦ï¼‰
    hof_threshold = 20  # éœå¤«å˜æ¢çš„ç´¯åŠ é˜ˆå€¼
    min_line_len = 30  # éœå¤«å˜æ¢æ£€æµ‹çš„æœ€å°çº¿æ®µé•¿åº¦
    max_line_gap = 60  # éœå¤«å˜æ¢å…è®¸çš„æœ€å¤§çº¿æ®µæ–­è£‚é•¿åº¦
    kernel_size = 5  # é«˜æ–¯æ¨¡ç³Šçš„æ ¸å¤§å°
    canny_low_threshold = 75  # Canny è¾¹ç¼˜æ£€æµ‹çš„ä½é˜ˆå€¼
    canny_high_threshold = canny_low_threshold * 3  
    # Canny è¾¹ç¼˜æ£€æµ‹çš„é«˜é˜ˆå€¼ï¼ˆä½é˜ˆå€¼çš„ 3 å€ï¼‰
    alpha, beta, lambda_ = 0.8, 1., 0.  # å›¾åƒèåˆçš„æƒé‡å‚æ•°
    imshape = image.shape  # è·å–è¾“å…¥å›¾åƒçš„å°ºå¯¸ï¼ˆé«˜åº¦ã€å®½åº¦ã€é€šé“æ•°ï¼‰
    # ç°åº¦å›¾è½¬æ¢
    gray = grayscale(image)
    # é«˜æ–¯æ»¤æ³¢
    blur_gray = gaussian_blur(gray, kernel_size)
    # Canny è¾¹ç¼˜æ£€æµ‹
    edge_image = canny(blur_gray, canny_low_threshold, canny_high_threshold)
    # ç”Ÿæˆæ©æ¨¡åŒºåŸŸ
    vertices = np.array([[(0, imshape[0]), 
            (9 * imshape[1] / 20, 11 * imshape[0] / 18),
            (11 * imshape[1] / 20, 11 * imshape[0] / 18), 
            (imshape[1], imshape[0])]], dtype=np.int32)
    
    masked_edges = region_of_interest(edge_image, vertices)  
    # æå–æ„Ÿå…´è¶£åŒºåŸŸï¼Œé™åˆ¶æ£€æµ‹èŒƒå›´
    # éœå¤«å˜æ¢æ£€æµ‹ç›´çº¿
    lines = hough_lines(masked_edges, rho, theta, 
        hof_threshold, min_line_len, max_line_gap)
    line_image = np.zeros_like(image)  
    # åˆ›å»ºä¸åŸå›¾å¤§å°ç›¸åŒçš„ç©ºç™½å›¾åƒï¼Œç”¨äºç»˜åˆ¶è½¦é“çº¿
    # ç»˜åˆ¶è½¦é“çº¿
    line_image = draw_lines(line_image, lines)
    # å›¾åƒèåˆ
    lines_edges = weighted_img(line_image, image, 
                                alpha, beta, lambda_)

    return lines_edges  # è¿”å›å¤„ç†åçš„å›¾åƒ

if __name__ == '__main__':
    image = cv.imread('jupyter/test1.png')  
    line_image = process_image(image)  # è°ƒç”¨ä¸»å‡½æ•°å¤„ç†å›¾åƒ
    cv.imwrite('result/task8/resultline_img.jpg', line_image)  
    </code></pre>


    <h2>é™„åŠ å®éªŒ1: SIFT ç‰¹å¾åŒ¹é…ä¸å›¾åƒæ‹¼æ¥</h2>
<div class="image-grid-container">
    <div class="image-grid-item">
        <img src="/CV_Class/labs/lab1/pic/task_add_1/sift_keypoints.jpg" alt="SIFT ç‰¹å¾åŒ¹é…ä¸å›¾åƒæ‹¼æ¥">
    </div>
    <div class="image-grid-item">
        <img src="/CV_Class/labs/lab1/pic/task_add_1/sift_matches.jpg" alt="SIFT ç‰¹å¾åŒ¹é…ä¸å›¾åƒæ‹¼æ¥">
    </div>
    <div class="image-grid-item">
        <img src="/CV_Class/labs/lab1/pic/task_add_1/sift_good_matches.jpg" alt="SIFT ç‰¹å¾åŒ¹é…ä¸å›¾åƒæ‹¼æ¥">
    </div>
    <div class="image-grid-item">
        <img src="/CV_Class/labs/lab1/pic/task_add_1/stitched_image.jpg" alt="SIFT ç‰¹å¾åŒ¹é…ä¸å›¾åƒæ‹¼æ¥">
    </div>
</div>
    <pre><code class="language-python">
import cv2 as cv
import numpy as np 

def get_homo(img1, img2):
    """è®¡ç®—ä¸¤å¹…å›¾åƒä¹‹é—´çš„å•åº”æ€§çŸ©é˜µ"""
    sift = cv.SIFT_create()  # åˆ›å»º SIFT ç‰¹å¾æ£€æµ‹å™¨ï¼Œç”¨äºæå–å›¾åƒç‰¹å¾ç‚¹
    
    # æå–å›¾åƒçš„å…³é”®ç‚¹å’Œæè¿°å­
    k1, des1 = sift.detectAndCompute(img1, None)
    k2, des2 = sift.detectAndCompute(img2, None)
    
    # åœ¨å›¾åƒä¸Šç»˜åˆ¶å…³é”®ç‚¹å¹¶ä¿å­˜
    img1_keypoints = cv.drawKeypoints(img1, k1, None)
    cv.imwrite("result/task_add_1/sift_keypoints.jpg", img1_keypoints)

    # åˆ›å»ºæš´åŠ›åŒ¹é…å™¨ï¼Œç”¨äºåŒ¹é…ä¸¤å¹…å›¾åƒçš„ç‰¹å¾ç‚¹
    bf = cv.BFMatcher()
    
    # ä½¿ç”¨ KNN ç®—æ³•åŒ¹é…æè¿°å­ï¼Œk=2 è¡¨ç¤ºæ¯ä¸ªç‚¹æ‰¾ä¸¤ä¸ªæœ€è¿‘é‚»
    matches = bf.knnMatch(des1, des2, k=2)
    
    # æå–æ‰€æœ‰åŒ¹é…å¯¹çš„ç¬¬ä¸€ä¸ªé‚»å±…
    all_matches = [m[0] for m in matches]
    
    # ç»˜åˆ¶æ‰€æœ‰åŒ¹é…å¹¶ä¿å­˜ç»“æœ
    match_img = cv.drawMatches(img1, k1, img2, k2, 
        all_matches, None, 
        flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
    cv.imwrite("result/task_add_1/sift_matches.jpg", match_img)

    # ç­›é€‰ä¼˜è´¨åŒ¹é…ï¼Œä½¿ç”¨ Lowe's ratio test
    verify_ratio = 0.8  # åŒ¹é…æ¯”ç‡é˜ˆå€¼ï¼Œåˆ¤æ–­åŒ¹é…è´¨é‡
    verify_matches = []
    for m1, m2 in matches:
        if m1.distance < verify_ratio * m2.distance:  
        # å¦‚æœç¬¬ä¸€ä¸ªé‚»å±…è·ç¦»å°äºç¬¬äºŒä¸ªçš„ 0.8 å€ï¼Œåˆ™ä¿ç•™
            verify_matches.append(m1)
    
    # ç»˜åˆ¶ä¼˜è´¨åŒ¹é…å¹¶ä¿å­˜
    good_matches_img = cv.drawMatches(img1, k1, img2, k2, 
        verify_matches, None, 
        flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
    cv.imwrite("result/task_add_1/sift_good_matches.jpg", good_matches_img) 
    
    # æ£€æŸ¥åŒ¹é…ç‚¹æ•°æ˜¯å¦è¶³å¤Ÿè®¡ç®—å•åº”æ€§çŸ©é˜µ
    min_matches = 8  # æœ€å°åŒ¹é…ç‚¹æ•°è¦æ±‚
    if len(verify_matches) > min_matches:
        # æå–åŒ¹é…ç‚¹çš„åæ ‡
        img1_pts = np.float32([k1[m.queryIdx].pt for m in verify_matches]).reshape(-1, 2)
        img2_pts = np.float32([k2[m.trainIdx].pt for m in verify_matches]).reshape(-1, 2)

        # ä½¿ç”¨ RANSAC ç®—æ³•è®¡ç®—å•åº”æ€§çŸ©é˜µ
        H, _ = cv.findHomography(img1_pts, img2_pts, cv.RANSAC, 5.0)
        return H 
    else:
        print(f"Error: Insufficient matches ({len(verify_matches)} < {min_matches})")  
        # è¾“å‡ºé”™è¯¯ä¿¡æ¯
        exit()

def stitch_image(img1, img2, H):
    """æ ¹æ®å•åº”æ€§çŸ©é˜µæ‹¼æ¥ä¸¤å¹…å›¾åƒ"""
    h1, w1 = img1.shape[:2]  
    h2, w2 = img2.shape[:2]
    
    # å®šä¹‰ä¸¤å¹…å›¾åƒçš„è§’ç‚¹åæ ‡
    img1_dims = np.float32([[0,0], [0,h1-1], 
                            [w1-1,h1-1], [w1-1,0]]).reshape(-1,1,2) 
    img2_dims = np.float32([[0,0], [0,h2-1], 
                            [w2-1,h2-1], [w2-1,0]]).reshape(-1,1,2)
    
    # å¯¹ç¬¬ä¸€å¹…å›¾åƒçš„è§’ç‚¹åº”ç”¨å•åº”æ€§å˜æ¢
    img1_transform = cv.perspectiveTransform(img1_dims, H)
    result = np.concatenate([img2_dims, img1_transform], axis=0)  
    # åˆå¹¶ä¸¤å¹…å›¾åƒçš„è§’ç‚¹åæ ‡
    
    [x_min, y_min] = np.int32(np.floor(result.min(axis=0).ravel())) 
    [x_max, y_max] = np.int32(np.ceil(result.max(axis=0).ravel())) 
    transform_dist = [-x_min, -y_min]  # è®¡ç®—å¹³ç§»è·ç¦»ä»¥é¿å…è´Ÿåæ ‡
    
    # åˆ›å»ºå¹³ç§»çŸ©é˜µï¼Œç”¨äºè°ƒæ•´å›¾åƒä½ç½®
    transform_array = np.array([[1,0,transform_dist[0]], 
                                [0,1,transform_dist[1]], [0,0,1]])
    
    # ç»„åˆå•åº”æ€§çŸ©é˜µå’Œå¹³ç§»çŸ©é˜µ
    output_shape = (x_max-x_min, y_max-y_min)
    H_output = transform_array @ H
    
    # å¯¹ç¬¬ä¸€å¹…å›¾åƒåº”ç”¨é€è§†å˜æ¢å¹¶æ‹¼æ¥ç¬¬äºŒå¹…å›¾åƒ
    result_img = cv.warpPerspective(img1, H_output, output_shape)
    result_img[transform_dist[1]:transform_dist[1]+h2, 
            transform_dist[0]:transform_dist[0]+w2] = img2
    return result_img  # è¿”å›æ‹¼æ¥åçš„å›¾åƒ

# ä¸»ç¨‹åº
images = []
for i in range(3):
    filename = 'jupyter/image{}.jpg'.format(i+1)
    img = cv.imread(filename)
    img = cv.resize(img,(640,480))
    images.append(img)

for i in range(1, len(images)):
    H = get_homo(images[i], images[i-1])
    images[i] = stitch_image(images[i], images[i-1], H)
    
    
res = images[len(images)-1]
cv.imwrite("result/task_add_1/stitched_image.jpg", res)
    </code></pre>

    <h2>é™„åŠ å®éªŒ2: ç»†èƒè½®å»“è¯†åˆ«ä¸ç¼–å·æ ‡æ³¨</h2>
    <div class="image-grid-container">
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task_add_2/gaussian.jpg" alt="é«˜æ–¯æ»¤æ³¢">
        </div>
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab1/pic/task_add_2/draw.jpg" alt="ç»†èƒè½®å»“è¯†åˆ«ä¸ç¼–å·æ ‡æ³¨">
        </div>
    </div>

    <pre><code class="language-python">
import cv2  
img = cv2.imread('jupyter/count.jpg', 1)

# å°†å½©è‰²å›¾åƒè½¬æ¢ä¸ºç°åº¦å›¾åƒ
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 

# å¯¹ç°åº¦å›¾åƒè¿›è¡Œé˜ˆå€¼å¤„ç†ï¼Œ150 ä¸ºé˜ˆå€¼ï¼Œ255 ä¸ºæœ€å¤§å€¼ï¼ŒTHRESH_BINARY_INV è¡¨ç¤ºåè½¬äºŒå€¼åŒ–
_, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)

# åˆ›å»ºä¸€ä¸ª 5x5 çš„æ¤­åœ†å½¢ç»“æ„å…ƒç´ ï¼Œç”¨äºå½¢æ€å­¦æ“ä½œ
kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))

# ä½¿ç”¨è…èš€æ“ä½œå»é™¤å›¾åƒä¸­çš„å™ªå£°
eroded = cv2.erode(binary, kernel, iterations=4)

# å¯¹è…èš€åçš„å›¾åƒè¿›è¡Œè†¨èƒ€
dilated = cv2.dilate(eroded, kernel, iterations=2)

# ä½¿ç”¨ 3x3 é«˜æ–¯æ ¸å¯¹å›¾åƒè¿›è¡Œæ¨¡ç³Šå¤„ç†ï¼Œå¹³æ»‘è¾¹ç¼˜ï¼Œå‡å°‘å™ªå£°å½±å“
gaussian = cv2.GaussianBlur(dilated, (3, 3), 0)

# æ£€æµ‹å›¾åƒä¸­çš„è½®å»“
contours, _ = cv2.findContours(gaussian, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

contoursOK = [] 
for i in contours:  # éå†æ‰€æœ‰æ£€æµ‹åˆ°çš„è½®å»“
    if cv2.contourArea(i) > 30:  # å¦‚æœè½®å»“é¢ç§¯å¤§äº 30ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ„ä¹‰çš„è½®å»“
        contoursOK.append(i) 

# åœ¨åŸå›¾åƒä¸Šç»˜åˆ¶ç»¿è‰²è½®å»“
draw = cv2.drawContours(img, contoursOK, -1, (0, 255, 0), 2)

# éå†ç­›é€‰åçš„è½®å»“åŠå…¶ç´¢å¼•
for i, j in zip(contoursOK, range(len(contoursOK))):
    M = cv2.moments(i)  # è®¡ç®—å½“å‰è½®å»“çš„çŸ©ï¼Œç”¨äºåç»­è´¨å¿ƒè®¡ç®—
    cX = int(M["m10"] / M["m00"])  
    cY = int(M["m01"] / M["m00"])
    
    # åœ¨è´¨å¿ƒä½ç½®æ ‡æ³¨è½®å»“çš„ç¼–å·
    cv2.putText(img, str(j), (cX, cY), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 2)

# ä¿å­˜é«˜æ–¯æ¨¡ç³Šå¤„ç†åçš„å›¾åƒ
cv2.imwrite("result/task_add_2/gaussian.jpg", gaussian)

# ä¿å­˜ç»“æœå›¾åƒ 'draw.jpg'
cv2.imwrite("result/task_add_2/draw.jpg", draw)
    </code></pre>

</div>


<footer>
    <p>Â© Copyright Capital Normal University 2025</p>
</footer>
