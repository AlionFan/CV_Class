<div id="lab3">
    <h1>第三周实验</h1>
    <h2>resnet50.py</h2>  
    <pre><code class="language-python">
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import time
import copy

# ResNet50 模型定义
class Bottleneck(nn.Module):
    """残差块的实现"""
    expansion = 4
    
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, 
            out_channels, kernel_size=3, stride=stride, 
            padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.conv3 = nn.Conv2d(out_channels, 
            out_channels * self.expansion, 
            kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)
        return out

class ResNet50(nn.Module):
    def __init__(self, num_classes=1000):
        super(ResNet50, self).__init__()
        
        # 初始卷积层和池化层
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, 
            stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, 
            stride=2, padding=1)
        
        # Layer 1 (conv2_x): 3个Bottleneck模块
        self.layer1 = nn.Sequential(
            Bottleneck(64, 64, stride=1, downsample=nn.Sequential(
                nn.Conv2d(64, 256, kernel_size=1, stride=1, bias=False),
                nn.BatchNorm2d(256)
            )),
            Bottleneck(256, 64),
            Bottleneck(256, 64)
        )
        
        # Layer 2 (conv3_x): 4个Bottleneck模块
        self.layer2 = nn.Sequential(
            Bottleneck(256, 128, stride=2, downsample=nn.Sequential(
                nn.Conv2d(256, 512, kernel_size=1, stride=2, bias=False),
                nn.BatchNorm2d(512)
            )),
            Bottleneck(512, 128),
            Bottleneck(512, 128),
            Bottleneck(512, 128)
        )
        
        # Layer 3 (conv4_x): 6个Bottleneck模块
        self.layer3 = nn.Sequential(
            Bottleneck(512, 256, stride=2, downsample=nn.Sequential(
                nn.Conv2d(512, 1024, kernel_size=1, stride=2, bias=False),
                nn.BatchNorm2d(1024)
            )),
            Bottleneck(1024, 256),
            Bottleneck(1024, 256),
            Bottleneck(1024, 256),
            Bottleneck(1024, 256),
            Bottleneck(1024, 256)
        )
        
        # Layer 4 (conv5_x): 3个Bottleneck模块
        self.layer4 = nn.Sequential(
            Bottleneck(1024, 512, stride=2, downsample=nn.Sequential(
                nn.Conv2d(1024, 2048, kernel_size=1, stride=2, bias=False),
                nn.BatchNorm2d(2048)
            )),
            Bottleneck(2048, 512),
            Bottleneck(2048, 512)
        )
        
        # 分类器
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(2048, num_classes)
        
        # 权重初始化
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        
        return x
# 创建模型实例
def create_resnet50(num_classes=102, pretrained=False):
    model = ResNet50(num_classes=num_classes)
    if pretrained:
        # 加载预训练权重
        state_dict = torch.load('resnet50-0676ba61.pth')
        model.load_state_dict(state_dict)
    return model
# 数据加载和转换函数
def load_flower_dataset(data_dir, batch_size=32):
    # 数据转换
    data_transforms = {
        'train': transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'valid': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }
    
    # 创建数据集
    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, 
        x), data_transforms[x]) 
                     for x in ['train', 'valid']}
    
    # 创建数据加载器
    dataloaders = {x: DataLoader(image_datasets[x], 
        batch_size=batch_size, shuffle=True, num_workers=4) 
                  for x in ['train', 'valid']}
    
    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}
    class_names = image_datasets['train'].classes
    
    return dataloaders, dataset_sizes, class_names

# 训练和验证函数
def train_model(model, dataloaders, 
            dataset_sizes, criterion, 
            optimizer, scheduler, 
            device, num_epochs=25):
    since = time.time()
    
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    
    # 记录训练过程
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': []
    }
    
    for epoch in range(num_epochs):
        print(f'Epoch {epoch+1}/{num_epochs}')
        print('-' * 10)
        
        # 每个epoch有训练和验证阶段
        for phase in ['train', 'valid']:
            if phase == 'train':
                model.train()  # 设置模型为训练模式
            else:
                model.eval()   # 设置模型为评估模式
                
            running_loss = 0.0
            running_corrects = 0
            
            # 迭代数据
            for inputs, labels in tqdm(dataloaders[phase]):
                inputs = inputs.to(device)
                labels = labels.to(device)
                
                # 梯度清零
                optimizer.zero_grad()
                
                # 前向传播
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                    
                    # 如果是训练阶段，则反向传播 + 优化
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                        
                # 统计
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            
            if phase == 'train' and scheduler is not None:
                scheduler.step()
                
            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects / dataset_sizes[phase]
            
            # 记录历史
            if phase == 'train':
                history['train_loss'].append(epoch_loss)
                history['train_acc'].append(epoch_acc.item())
            else:
                history['val_loss'].append(epoch_loss)
                history['val_acc'].append(epoch_acc.item())
            
            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
            
            # 深度复制模型
            if phase == 'valid' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
                
        print()
    
    # 打印训练耗时
    time_elapsed = time.time() - since
    print(f'Training complete in 
        {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'Best val Acc: {best_acc:.4f}')
    
    # 加载最佳模型权重
    model.load_state_dict(best_model_wts)
    return model, history

# 绘制训练曲线
def plot_training_history(history):
    plt.figure(figsize=(12, 4))
    
    # 损失曲线
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Training Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    
    # 准确率曲线
    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Training Accuracy')
    plt.plot(history['val_acc'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('./result/training_history.png')

# 测试模型
def test_model(model, dataloader, dataset_size, device):
    model.eval()
    running_corrects = 0
    
    # 不计算梯度
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            # 前向传播
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            
            # 统计正确个数
            running_corrects += torch.sum(preds == labels.data)
    
    # 计算准确率
    acc = running_corrects / dataset_size
    print(f'Testing Accuracy: {acc:.4f}')
    return acc

# 主函数
def main():
    # 设置参数
    data_dir = '../lab2/vision/data/flower_data/'  # 数据集路径
    num_classes = 102  # 有102个花类别
    batch_size = 32
    num_epochs = 3
    learning_rate = 0.001
    
    # 检查是否有GPU
    device = torch.device("cuda:0" if torch.cuda.is_available() else "mps")
    print(f"Using device: {device}")
    
    # 加载数据
    dataloaders, dataset_sizes, class_names = 
        load_flower_dataset(data_dir, batch_size)
    print(f"Loaded {len(class_names)} classes: {class_names}")
    print(f"Training set size: {dataset_sizes['train']}")
    print(f"Validation set size: {dataset_sizes['valid']}")
    
    # 创建模型
    model = create_resnet50(num_classes=num_classes, pretrained=False)
    model = model.to(device)
    
    # 定义损失函数和优化器
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)
    
    # 学习率调度器
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    
    # 训练模型
    model, history = train_model(
        model, dataloaders, dataset_sizes, 
        criterion, optimizer, scheduler, 
        device, num_epochs
    )
    
    # 绘制训练历史
    plot_training_history(history)
    
    # 保存模型
    torch.save(model.state_dict(), './result/resnet50_flower_classifier.pth')
    print("Model saved to result/resnet50_flower_classifier.pth")
    
    # 在验证集上测试模型
    test_acc = test_model(model, 
        dataloaders['valid'], 
        dataset_sizes['valid'], 
        device)

if __name__ == '__main__':
    main()
    </code></pre>
    
    <h2>resnet50att.py</h2>
    <pre><code class="language-python">
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import time
import copy
import math

# 标准自注意力机制模块
class StandardSelfAttention(nn.Module):
    """标准自注意力模块，类似Transformer中的自注意力机制
    
    该模块实现了多头自注意力机制，可以让模型关注输入特征的不同方面。
    通过并行的多个注意力头，增强了模型的表示能力。
    
    参数:
        in_channels (int): 输入特征通道数
        key_channels (int, optional): key的通道数，默认为输入通道数的一半
        value_channels (int, optional): value的通道数，默认等于输入通道数
        head_count (int): 注意力头的数量，默认为1
    """
    def __init__(self, in_channels, key_channels=None, 
                value_channels=None, head_count=1):
        super(StandardSelfAttention, self).__init__()
        # 如果未指定key_channels，则设为输入通道数的一半
        if key_channels is None:
            key_channels = in_channels // 2
        # 如果未指定value_channels，则设为输入通道数
        if value_channels is None:
            value_channels = in_channels
        
            
        self.in_channels = in_channels
        self.key_channels = key_channels
        self.head_count = head_count
        self.value_channels = value_channels
        
        # 定义三个1x1卷积，分别用于生成queries、keys和values
        self.keys = nn.Conv2d(in_channels, key_channels, kernel_size=1)
        self.queries = nn.Conv2d(in_channels, key_channels, kernel_size=1)
        self.values = nn.Conv2d(in_channels, value_channels, kernel_size=1)
        # 输出通道数都乘以head_count以支持多头注意力
        out_channels = self.head_count * value_channels

        # 用于将多头注意力的结果投影回原始维度
        self.reprojection = nn.Conv2d(value_channels, out_channels, kernel_size=1)
        
        # softmax用于计算注意力权重
        self.softmax = nn.Softmax(dim=-1)
        
        # 使用He初始化所有卷积层的权重，以提高训练的稳定性
        nn.init.kaiming_normal_(self.keys.weight)
        nn.init.kaiming_normal_(self.queries.weight)
        nn.init.kaiming_normal_(self.values.weight)
        nn.init.kaiming_normal_(self.reprojection.weight)
        
    def forward(self, x):
        """前向传播函数
        
        参数:
            x: 输入特征图，形状为 [batch_size, in_channels, height, width]
            
        返回:
            out: 经过自注意力处理的特征图，形状与输入相同
        """
        # 获取输入特征的形状
        batch_size, _, h, w = x.size()
        residual = x
        queries = self.queries(x).view(batch_size, self.head_count, 
                                        self.key_channels, h*w)
        keys = self.keys(x).view(batch_size, self.head_count, 
                                    self.key_channels, h*w)
        values = self.values(x).view(batch_size, self.head_count, 
                                        self.value_channels, h*w)

        keys = keys.permute(0, 1, 3, 2)
        queries = queries.permute(0, 1, 2, 3)

        scale_factor = math.sqrt(self.key_channels)
        attention = torch.matmul(keys, queries) / scale_factor
        attention = self.softmax(attention)

        values = values.permute(0, 1, 2, 3)
        out = torch.matmul(attention, values.permute(0, 1, 3, 2))
        out = out.permute(0, 1, 3, 2).contiguous().view(batch_size, -1, h, w)

        out = self.reprojection(out)
        out += residual
        return out

# 基本残差块，但没有注意力机制
class Bottleneck(nn.Module):
    """残差块的实现"""
    expansion = 4
    
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        
        # 1x1 卷积降维
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        # 3x3 卷积
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                              stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # 1x1 卷积升维
        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,
                              kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)
        
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample

    def forward(self, x):
        identity = x
        
        # 第一个卷积块
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        # 第二个卷积块
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        
        # 第三个卷积块
        out = self.conv3(out)
        out = self.bn3(out)
        
        # 残差连接
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity
        out = self.relu(out)
        
        return out

class ResNet50WithLayerAttention(nn.Module):
    def __init__(self, num_classes=1000):
        super(ResNet50WithLayerAttention, self).__init__()
        
        # 初始卷积层和池化层
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, 
                            stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # Layer 1 (conv2_x): 3个Bottleneck模块
        self.layer1 = nn.Sequential(
            Bottleneck(64, 64, stride=1, downsample=nn.Sequential(
                nn.Conv2d(64, 256, kernel_size=1, stride=1, bias=False),
                nn.BatchNorm2d(256)
            )),
            Bottleneck(256, 64),
            Bottleneck(256, 64)
        )
        
        # Layer 2 (conv3_x): 4个Bottleneck模块
        self.layer2 = nn.Sequential(
            Bottleneck(256, 128, stride=2, downsample=nn.Sequential(
                nn.Conv2d(256, 512, kernel_size=1, stride=2, bias=False),
                nn.BatchNorm2d(512)
            )),
            Bottleneck(512, 128),
            Bottleneck(512, 128),
            Bottleneck(512, 128)
        )
        
        # Layer 3 (conv4_x): 6个Bottleneck模块
        self.layer3 = nn.Sequential(
            Bottleneck(512, 256, stride=2, downsample=nn.Sequential(
                nn.Conv2d(512, 1024, kernel_size=1, stride=2, bias=False),
                nn.BatchNorm2d(1024)
            )),
            Bottleneck(1024, 256),
            Bottleneck(1024, 256),
            Bottleneck(1024, 256),
            Bottleneck(1024, 256),
            Bottleneck(1024, 256)
        )
        
        # Layer 4 (conv5_x): 3个Bottleneck模块
        self.layer4 = nn.Sequential(
            Bottleneck(1024, 512, stride=2, downsample=nn.Sequential(
                nn.Conv2d(1024, 2048, kernel_size=1, stride=2, bias=False),
                nn.BatchNorm2d(2048)
            )),
            Bottleneck(2048, 512),
            Bottleneck(2048, 512)
        )

        self.attention3 = StandardSelfAttention(1024)
        self.attention4 = StandardSelfAttention(2048)
        
        # 分类器
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(2048, num_classes)
        
        # 权重初始化
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, 
                    mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        # 应用残差层和对应的注意力模块
        x = self.layer1(x)        
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.attention3(x)
        
        x = self.layer4(x)
        x = self.attention4(x)
        
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        
        return x

# 创建带有每层一个自注意力的ResNet50模型
def create_resnet50_with_layer_attention(num_classes=102, pretrained=False):
    model = ResNet50WithLayerAttention(num_classes=num_classes)
    if pretrained:
        # 如果要从预训练模型加载权重，需要处理注意力层的缺失
        # 这里简化处理，实际使用中可能需要更复杂的权重迁移策略
        state_dict = torch.load('resnet50-0676ba61.pth')
        model_dict = model.state_dict()
        
        # 筛选出可以使用的权重
        pretrained_dict = {k: v for k, v in state_dict.items() 
            if k in model_dict and model_dict[k].size() == v.size()}
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)
        
        print(f"Loaded {len(pretrained_dict)}/{len(state_dict)} 
            pretrained parameters")
    return model

# 数据加载和转换函数
def load_flower_dataset(data_dir, batch_size=32):
    # 数据转换
    data_transforms = {
        'train': transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'valid': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }
    
    # 创建数据集
    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir,
        x), data_transforms[x]) 
                     for x in ['train', 'valid']}
    
    # 创建数据加载器
    dataloaders = {x: DataLoader(image_datasets[x], 
        batch_size=batch_size, shuffle=True, num_workers=4) 
                  for x in ['train', 'valid']}
    
    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}
    class_names = image_datasets['train'].classes
    
    return dataloaders, dataset_sizes, class_names

# 训练和验证函数
def train_model(model, dataloaders, dataset_sizes, 
                criterion, optimizer, scheduler, 
                device, num_epochs=25):
    since = time.time()
    
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    
    # 记录训练过程
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': []
    }
    
    for epoch in range(num_epochs):
        print(f'Epoch {epoch+1}/{num_epochs}')
        print('-' * 10)
        
        # 每个epoch有训练和验证阶段
        for phase in ['train', 'valid']:
            if phase == 'train':
                model.train()  # 设置模型为训练模式
            else:
                model.eval()   # 设置模型为评估模式
                
            running_loss = 0.0
            running_corrects = 0
            
            # 迭代数据
            for inputs, labels in tqdm(dataloaders[phase]):
                inputs = inputs.to(device)
                labels = labels.to(device)
                
                # 梯度清零
                optimizer.zero_grad()
                
                # 前向传播
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                    
                    # 如果是训练阶段，则反向传播 + 优化
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                        
                # 统计
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            
            if phase == 'train' and scheduler is not None:
                scheduler.step()
                
            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects / dataset_sizes[phase]
            
            # 记录历史
            if phase == 'train':
                history['train_loss'].append(epoch_loss)
                history['train_acc'].append(epoch_acc.item())
            else:
                history['val_loss'].append(epoch_loss)
                history['val_acc'].append(epoch_acc.item())
            
            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
            
            # 深度复制模型
            if phase == 'valid' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
                
        print()
    
    # 打印训练耗时
    time_elapsed = time.time() - since
    print(f'Training complete in 
        {time_elapsed // 60:.0f}m 
        {time_elapsed % 60:.0f}s')
    print(f'Best val Acc: {best_acc:.4f}')
    
    # 加载最佳模型权重
    model.load_state_dict(best_model_wts)
    return model, history

# 绘制训练曲线
def plot_training_history(history):
    plt.figure(figsize=(12, 4))
    
    # 损失曲线
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Training Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    
    # 准确率曲线
    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Training Accuracy')
    plt.plot(history['val_acc'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('./result/training_history_layer_attention.png')

# 测试模型
def test_model(model, dataloader, dataset_size, device):
    model.eval()
    running_corrects = 0
    
    # 不计算梯度
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            # 前向传播
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            
            # 统计正确个数
            running_corrects += torch.sum(preds == labels.data)
    
    # 计算准确率
    acc = running_corrects / dataset_size
    print(f'Testing Accuracy: {acc:.4f}')
    return acc


# 主函数
def main():
    # 设置参数
    data_dir = '../lab2/vision/data/flower_data/'  # 数据集路径
    num_classes = 102  # 102个花类别
    batch_size = 4
    num_epochs = 1
    learning_rate = 0.001
    
    # 检查是否有GPU
    device = torch.device("cuda:0" if torch.cuda.is_available() else "mps")
    print(f"Using device: {device}")
    
    # 加载数据
    dataloaders, dataset_sizes, class_names = 
        load_flower_dataset(data_dir, batch_size)
    print(f"Loaded {len(class_names)} classes: {class_names}")
    print(f"Training set size: {dataset_sizes['train']}")
    print(f"Validation set size: {dataset_sizes['valid']}")
    
    # 创建带有层级自注意力的ResNet50模型
    model = create_resnet50_with_layer_attention
    (num_classes=num_classes, pretrained=False)
    model = model.to(device)
    
    # 定义损失函数和优化器
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)
    
    # 学习率调度器
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    
    # 训练模型
    model, history = train_model(
        model, dataloaders, dataset_sizes, 
        criterion, optimizer, scheduler, device, num_epochs
    )
    
    # 绘制训练历史
    plot_training_history(history)
    
    # 保存模型
    torch.save(model.state_dict(), 
    './result/resnet50_layer_attention_flower_classifier.pth')
    print("Model saved to 
        result/resnet50_layer_attention_flower_classifier.pth")
    
    # 在验证集上测试模型
    test_acc = test_model(model, 
        dataloaders['valid'], dataset_sizes['valid'], device)

if __name__ == '__main__':
    main()
    </code></pre>


    <h2>3.py</h2>
    <div class="image-grid-container">
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab3/pic/result3/layer1_features.png">
        </div>
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab3/pic/result3/layer2_features.png">
        </div>
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab3/pic/result3/layer3_features.png">
        </div>
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab3/pic/result3/layer4_features.png">
        </div>
    </div>
    <pre><code class="language-python">
import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import os
from resnet50 import create_resnet50

# 创建结果保存目录
if not os.path.exists("result/result3"):
    os.makedirs("result/result3", exist_ok=True)

# 加载预训练的ResNet-50模型并设置为评估模式
model = create_resnet50()

weights_path = "./result/resnet50_flower_classifier.pth"

# 加载权重
model.load_state_dict(torch.load(weights_path))

model.eval()

# 用于存储各层特征图的字典
features = {}
# 定义hook函数来捕获层输出
def get_hook(name):
    def hook_feature(module, input, output):
        features[name] = output
    return hook_feature

# 选择要观察的网络层
layers = {
    'layer1': model.layer1,
    'layer2': model.layer2,
    'layer3': model.layer3,
    'layer4': model.layer4
}

# 为各层注册hook
for name, layer in layers.items():
    layer.register_forward_hook(get_hook(name))

# 定义图像预处理流程
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # 调整图像大小
    transforms.ToTensor(),  # 转换为张量
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
      # 标准化
])

# 加载并预处理图像
def load_image(image_path):
    img = Image.open(image_path).convert('RGB')  # 打开并转换为RGB格式
    img = transform(img).unsqueeze(0)  # 应用变换并增加batch维度
    return img

# 可视化特征图
def visualize_feature_map(feature_map, layer_name):
    # 移除batch维度
    feature_map = feature_map.squeeze(0)
    # 最多显示16个特征图
    num_feature_maps = min(feature_map.size(0), 16)
    
    # 创建4x4子图
    fig, axes = plt.subplots(4, 4, figsize=(12, 12))
    for idx in range(num_feature_maps):
        ax = axes[idx//4, idx%4]
        # 显示特征图
        channel = feature_map[idx].detach().numpy()
        ax.imshow(channel, cmap='viridis')
        ax.axis('off')  # 关闭坐标轴
    
    # 调整子图间距并保存
    plt.subplots_adjust(wspace=0.1, hspace=0.1)
    save_path = os.path.join("result/result3", f"{layer_name}_features.png")
    plt.savefig(save_path, dpi=100, bbox_inches='tight')
    plt.close()

# 主程序
if __name__ == "__main__":
    image_path = "../lab2/vision/data/flower_data/valid/1/image_06739.jpg"  
    # 输入图像路径
    input_tensor = load_image(image_path)  # 加载图像
    
    with torch.no_grad():  # 不计算梯度
        _ = model(input_tensor)  # 前向传播
    
    # 可视化各层特征图
    for name, feature in features.items():
        visualize_feature_map(feature, name)
    </code></pre>

    <h2>4.py</h2>
    <div class="image-grid-container">
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab3/pic/result4/all_channel_activations_histogram.png">
        </div>
        <div class="image-grid-item">
            <img src="/CV_Class/labs/lab3/pic/result4/top_channel_activations.png">
        </div>
    </div>

    <pre><code class="language-python">
import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import os
from resnet50 import create_resnet50

# 创建结果保存目录
if not os.path.exists("result/result4"):
    os.makedirs("result/result4", exist_ok=True)

# 加载预训练的ResNet-50模型并设置为评估模式
model = create_resnet50()

weights_path = "./result/resnet50_flower_classifier.pth"

# 加载权重
model.load_state_dict(torch.load(weights_path))
model.eval()

# 用于存储激活值的列表
activations = []

# 定义hook函数来捕获层输出
def hook(module, input, output):
    activations.append(output)
# 注册hook到layer1
model.layer1.register_forward_hook(hook)

# 定义图像预处理流程
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # 调整图像大小
    transforms.ToTensor(),  # 转换为张量
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
      # 标准化
])

# 加载并预处理图像
def load_image(image_path):
    img = Image.open(image_path).convert('RGB')  # 打开并转换为RGB格式
    img = transform(img).unsqueeze(0)  # 应用变换并增加batch维度
    return img

# 可视化激活值最高的top N个通道
def visualize_top_channel_activations(activation, top_n=10):
    # 按平均激活值降序排序，获取top N的通道索引
    channel_means = torch.mean(activation, dim=(2, 3)).squeeze(0)
    sorted_indices = torch.argsort(channel_means, descending=True)
    top_indices = sorted_indices[:top_n].tolist()
    
    # 绘制top N个通道的激活图
    fig, axes = plt.subplots(1, top_n, figsize=(15, 3))
    for i, idx in enumerate(top_indices):
        channel_activation = activation[0, idx].detach().numpy().squeeze()
        axes[i].imshow(channel_activation, cmap='viridis')
        axes[i].axis('off')
    plt.tight_layout()
    plt.savefig("./result/result4/top_channel_activations.png")
    plt.close()
    # 绘制并保存所有通道激活值的直方图
    plt.figure(figsize=(10, 6))
    plt.bar(range(len(channel_means)), 
        channel_means, color='blue', 
        alpha=0.7, label='Channel Activations')
    plt.xlabel('Channel Index')
    plt.ylabel('Mean Activation')
    plt.title('Channel-wise Activation Statistics')
    plt.legend()
    plt.savefig("result/result4/all_channel_activations_histogram.png")
    plt.close()
    

# 主程序
if __name__ == "__main__":
    image_path = "../lab2/vision/data/flower_data/valid/1/image_06739.jpg"  
    # 输入图像路径
    input_tensor = load_image(image_path)  # 加载图像
    
    with torch.no_grad():  # 不计算梯度
        _ = model(input_tensor)  # 前向传播
    
    # 可视化layer1的top 10通道激活
    visualize_top_channel_activations(activations[0], top_n=10)
    </code></pre>

    <h2>5.py</h2>
    <div class="image-grid-item">
        <img src="/CV_Class/labs/lab3/pic/result5/resnet_gradcam.png">
    </div>
    <pre><code class="language-python">
import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import cv2
import os
from resnet50att import create_resnet50_with_layer_attention

# 创建结果保存目录
if not os.path.exists("result/result5"):
    os.makedirs("result/result5", exist_ok=True)

class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        
        # 注册前向钩子
        self.target_layer.register_forward_hook(self.save_activation)
        # 更新反向钩子注册方式
        self.target_layer.register_full_backward_hook(self.save_gradient)

    def save_activation(self, module, input, output):
        self.activations = output  # 保存目标层的激活值
    
    def save_gradient(self, module, grad_in, grad_out):
        self.gradients = grad_out[0]  # 保存目标层的梯度
    
    def generate(self, input_image, target_class=None):
        # 设置模型为评估模式
        self.model.eval()
        
        # 前向传播
        output = self.model(input_image)  # 获取模型输出
        
        # 若未指定类别，取预测最大值
        if target_class is None:
            target_class = output.argmax(dim=1)  # 自动获取预测类别
        
        # 清空梯度
        self.model.zero_grad()
        
        # 反向传播计算梯度
        one_hot = torch.zeros_like(output)
        one_hot[0][target_class] = 1  # 创建one-hot编码
        output.backward(gradient=one_hot, retain_graph=True)  # 反向传播
        
        # 计算梯度权重
        gradients = self.gradients.cpu().numpy()[0]  # 添加cpu()转换
        activations = self.activations.detach().cpu().numpy()[0]  # 保持detach()
        weights = np.mean(gradients, axis=(1, 2))  # 全局平均池化
        
        # 生成 CAM
        cam = np.zeros(activations.shape[1:], dtype=np.float32)
        for i, w in enumerate(weights):
            cam += w * activations[i]  # 加权求和
        
        # 应用 ReLU 激活
        cam = np.maximum(cam, 0)  # 过滤负值
        
        # 归一化处理
        cam = cv2.normalize(cam, None, alpha=0, beta=1, 
                           norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)
        # 返回处理后的CAM
        return cam  # 直接返回numpy数组，无需detach()

# 定义图像预处理流程
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # 调整图像大小
    transforms.ToTensor(),  # 转换为张量
    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # 标准化
                        std=[0.229, 0.224, 0.225])
])

if __name__ == "__main__":
    # 加载预训练 ResNet50自注意力模型
    model = create_resnet50_with_layer_attention()

    weights_path = "result/resnet50_layer_attention_flower_classifier.pth" 
    
    # 加载权重
    model.load_state_dict(torch.load(weights_path))

    grad_cam = GradCAM(model, model.attention4)  # 初始化 Grad-CAM，指定目标层
    
    # 加载并预处理图像
    image_path = "../lab2/vision/data/flower_data/valid/1/image_06739.jpg"  
    # 输入图像路径
    img = Image.open(image_path).convert('RGB')  # 打开并转换为 RGB
    input_tensor = transform(img).unsqueeze(0)  # 预处理并增加批次维度
    
    # 生成 Grad-CAM 可视化
    cam = grad_cam.generate(input_tensor)  # 计算 Grad-CAM
    
    # 将原始图像调整为 224x224 并转换为 numpy 数组
    img = np.array(img.resize((224, 224)))
    
    # 生成热力图并调整大小
    heatmap = cv2.resize(cam, (224, 224))  # 调整热力图大小
    heatmap = np.uint8(255 * heatmap)  # 转换为 0-255 范围
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)  # 生成彩色热力图
    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)  # 转换为 RGB 格式
    
    # 叠加热力图和原始图像
    alpha = 0.4  # 热力图透明度
    superimposed_img = heatmap * alpha + img * (1 - alpha)  # 加权融合
    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)  
    # 裁剪值域
    
    # 显示并保存结果
    plt.imshow(superimposed_img)  # 显示叠加图像
    plt.axis('off')  # 关闭坐标轴
    plt.savefig("result/result5/resnet_gradcam.png", bbox_inches='tight')  
    # 保存结果
    plt.close()  # 关闭图像
    </code></pre>

    <h2>deit.py</h2>
    <pre><code class="language-python">
import torch
import torch.nn as nn
from functools import partial

# 辅助函数和类
def drop_path(x, drop_prob: float = 0., training: bool = False):
    """随机深度实现函数（Stochastic Depth）
    
    在训练深层网络时，随机让某些层的输出为0，类似Dropout的思想，但是应用在整个层上。
    这种方法可以提高深层网络的训练效率和泛化能力。
    
    参数:
        x: 输入张量
        drop_prob: 丢弃概率（0-1之间）
        training: 是否在训练模式
    
    返回:
        若在训练模式下，返回经过随机深度处理的张量；否则返回原始输入
    """
    # 如果丢弃概率为0或非训练状态，直接返回输入
    if drop_prob == 0. or not training:
        return x
    
    # 计算保留概率
    keep_prob = 1 - drop_prob
    
    # 创建随机mask的形状：保持batch维度，其他维度均为1
    # 例如输入shape为(N, C, H, W)，则mask的shape为(N, 1, 1, 1)
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)
    
    # 生成随机张量并二值化（保留或丢弃）
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # 向下取整，得到0或1
    
    # 缩放输出以保持期望值不变
    output = x.div(keep_prob) * random_tensor
    return output

class DropPath(nn.Module):
    """DropPath模块（随机深度的封装类）
    
    将random depth包装成PyTorch模块的形式，方便在网络中使用。
    
    参数:
        drop_prob: 层的丢弃概率
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        """前向传播函数"""
        return drop_path(x, self.drop_prob, self.training)

class Mlp(nn.Module):
    """多层感知机模块
    
    实现了一个两层的MLP结构，包含非线性激活函数和Dropout。
    这种结构常用于Transformer和ViT等网络中的FFN（Feed-Forward Network）部分。
    
    参数:
        in_features: 输入特征维度
        hidden_features: 隐藏层特征维度，默认等于输入维度
        out_features: 输出特征维度，默认等于输入维度
        act_layer: 激活函数，默认使用GELU
        drop: Dropout比率
    """
    def __init__(self, in_features, 
        hidden_features=None, out_features=None, 
        act_layer=nn.GELU, drop=0.):
        super().__init__()
        # 如果未指定输出维度，则使用输入维度
        out_features = out_features or in_features
        # 如果未指定隐藏层维度，则使用输入维度
        hidden_features = hidden_features or in_features
        
        # 第一个全连接层：输入维度 -> 隐藏维度
        self.fc1 = nn.Linear(in_features, hidden_features)
        # 非线性激活函数
        self.act = act_layer()
        # 第二个全连接层：隐藏维度 -> 输出维度
        self.fc2 = nn.Linear(hidden_features, out_features)
        # Dropout层，用于防止过拟合
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        """前向传播函数
        
        实现了: FC -> Act -> Drop -> FC -> Drop 的序列操作
        """
        x = self.fc1(x)  # 第一个全连接层
        x = self.act(x)  # 激活函数
        x = self.drop(x)  # Dropout
        x = self.fc2(x)  # 第二个全连接层
        x = self.drop(x)  # Dropout
        return x
    
class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, 
                qkv_bias=False, qk_scale=None, 
                attn_drop=0., proj_drop=0.):
        """多头自注意力机制实现
        Args:
            dim: 输入token的维度
            num_heads: 注意力头的数量，默认8头
            qkv_bias: 是否使用偏置项，默认False
            qk_scale: 缩放因子，若不指定则使用head_dim的根号值
            attn_drop: 注意力dropout率，默认0
            proj_drop: 输出dropout率，默认0
        """
        super().__init__()
        self.num_heads = num_heads
        # 每个注意力头的维度
        head_dim = dim // num_heads
        # 缩放因子，用于调节注意力分数，防止其过大导致softmax梯度消失
        self.scale = qk_scale or head_dim ** -0.5
        # 生成Q、K、V矩阵的线性变换，将输入变换为三倍维度(Q、K、V各占一份)
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        # 注意力权重的dropout层
        self.attn_drop = nn.Dropout(attn_drop)
        # 输出的线性变换层
        self.proj = nn.Linear(dim, dim)
        # 输出的dropout层
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        """前向传播过程
        Args:
            x: 输入tensor, 形状为 (B, N, C)
                B 是批次大小
                N 是序列长度
                C 是特征维度
        Returns:
            x: 输出tensor, 形状与输入相同 (B, N, C)
        """
        B, N, C = x.shape  # 新增维度提取
        x = self.qkv(x).reshape(B, N, 3, self.num_heads, 
            C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = x[0], x[1], x[2]
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class Block(nn.Module):
    def __init__(self, dim, num_heads, 
                mlp_ratio=4., qkv_bias=False, 
                qk_scale=None, drop=0., 
                attn_drop=0., drop_path=0., 
                act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, 
            qkv_bias=qkv_bias, 
            qk_scale=qk_scale, 
            attn_drop=attn_drop, 
            proj_drop=drop)
        self.drop_path = DropPath(drop_path) 
        if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, 
                        hidden_features=mlp_hidden_dim, 
                        act_layer=act_layer, 
                        drop=drop)

    def forward(self, x):
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x

class PatchEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.img_size = (img_size, img_size)
        self.patch_size = (patch_size, patch_size)
        self.num_patches = (img_size // patch_size) * (img_size // patch_size)
        self.proj = nn.Conv2d(in_chans, 
                                embed_dim, 
                                kernel_size=patch_size, 
                                stride=patch_size)

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x

class VisionTransformer(nn.Module):
    def __init__(self, img_size=224, 
                patch_size=16, in_chans=3, 
                num_classes=1000, embed_dim=768, 
                depth=12, num_heads=12, 
                mlp_ratio=4., qkv_bias=True, 
                qk_scale=None, drop_rate=0., 
                attn_drop_rate=0., 
                drop_path_rate=0., 
                norm_layer=partial(nn.LayerNorm, eps=1e-6), 
                distilled=False):
        super().__init__()
        self.num_classes = num_classes
        self.num_features = self.embed_dim = embed_dim
        self.distilled = distilled
        
        self.patch_embed = PatchEmbed(
            img_size=img_size, 
            patch_size=patch_size, 
            in_chans=in_chans, 
            embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) 
            if distilled else None
        self.pos_embed = nn.Parameter(torch.zeros(1, 
                num_patches + (2 if distilled else 1), 
                embed_dim))
        self.pos_drop = nn.Dropout(p=drop_rate)

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        self.blocks = nn.Sequential(*[
            Block(
                dim=embed_dim, num_heads=num_heads, 
                mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, 
                qk_scale=qk_scale, drop=drop_rate, 
                attn_drop=attn_drop_rate, drop_path=dpr[i],
                norm_layer=norm_layer)
            for i in range(depth)])
        self.norm = norm_layer(embed_dim)

        # Classifier head(s)
        if self.distilled:
            self.head = nn.Linear(embed_dim, num_classes) 
                if num_classes > 0 else nn.Identity()
            self.head_dist = nn.Linear(embed_dim, num_classes) 
                if num_classes > 0 else nn.Identity()
        else:
            self.head = nn.Linear(embed_dim, num_classes) 
                if num_classes > 0 else nn.Identity()

        # 初始化权重
        nn.init.trunc_normal_(self.pos_embed, std=.02)
        nn.init.trunc_normal_(self.cls_token, std=.02)
        if self.dist_token is not None:
            nn.init.trunc_normal_(self.dist_token, std=.02)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token', 'dist_token'}

    def forward_features(self, x):
        B = x.shape[0]
        x = self.patch_embed(x)

        cls_tokens = self.cls_token.expand(B, -1, -1)
        if self.dist_token is None:
            x = torch.cat((cls_tokens, x), dim=1)
        else:
            dist_token = self.dist_token.expand(B, -1, -1)
            x = torch.cat((cls_tokens, dist_token, x), dim=1)

        x = x + self.pos_embed
        x = self.pos_drop(x)

        x = self.blocks(x)
        x = self.norm(x)
        
        if self.dist_token is None:
            return self.head(x[:, 0])
        else:
            x_cls = self.head(x[:, 0])
            x_dist = self.head_dist(x[:, 1])
            return x_cls, x_dist

    def forward(self, x):
        x = self.forward_features(x)
        return x

# 创建DeiT-Tiny模型
def deit_tiny_patch16_224(pretrained=False, **kwargs):
    model = VisionTransformer(
        img_size=224,
        patch_size=16, 
        embed_dim=192, 
        depth=12, 
        num_heads=3, 
        mlp_ratio=4, 
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        distilled=False,  # 设为False以匹配预训练权重
        **kwargs
    )
    
    if pretrained:
        checkpoint = torch.hub.load_state_dict_from_url(
url="https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth",
            map_location="cpu", check_hash=True
        )
        model.load_state_dict(checkpoint["model"])
    
    return model

# 示例：创建模型并加载预训练权重（从本地文件）
def load_deit_tiny_from_local(path_to_weights):
    model = deit_tiny_patch16_224()
    
    checkpoint = torch.load(path_to_weights, map_location="cpu")
    if "model" in checkpoint:
        model.load_state_dict(checkpoint["model"])
    else:
        model.load_state_dict(checkpoint)
    
    return model

# 示例使用
if __name__ == "__main__":
    # 方法1：使用预训练参数创建模型（将自动下载权重）
    model = deit_tiny_patch16_224(pretrained=True)
    
    # 方法2：从本地文件加载权重
    # model = load_deit_tiny_from_local(
        "exp3/weights/deit_tiny_patch16_224-a1311bcf.pth")
    
    # 设置为评估模式
    model.eval()
    
    # 测试前向传播
    x = torch.randn(1, 3, 224, 224)
    with torch.no_grad():
        output = model(x)
    
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {output.shape}")
    </code></pre>


    <h2>7.py</h2>
    <div class="image-grid-item">
        <img src="/CV_Class/labs/lab3/pic/result7/gradcam_vit.jpg">
    </div>
    <pre><code class="language-python">
import cv2
import numpy as np
import torch
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image
from deit import load_deit_tiny_from_local
import os
from pytorch_grad_cam.base_cam import BaseCAM


if not os.path.exists("result/result7"):
    os.makedirs("result/result7", exist_ok=True)
# 直接指定图像路径和其他参数
image_path = '../lab2/vision/data/flower_data/valid/1/image_06739.jpg'
device = 'cuda'  # 使用CPU，也可以改为'cuda'使用GPU
aug_smooth = False  # 是否应用增强平滑
eigen_smooth = False  # 是否使用特征平滑

# 加载模型
model = load_deit_tiny_from_local("exp3/weights/deit_tiny_patch16_224-a1311bcf.pth")

weights_path = "exp3/weights/deit_tiny_patch16_224-a1311bcf.pth"


# 加载权重
model.load_state_dict(torch.load(weights_path)['model'])

# 选择目标层，这里使用最后一个norm层

# 创建适用于ViT的GradCAM子类
# 在GetGradCAM类中添加reshape_transform处理
class GetGradCAM(BaseCAM):
    def __init__(self, model, target_layers, use_cuda=False):
        super().__init__(model, target_layers, use_cuda)
        self.activations_and_grads.reshape_transform = 
        lambda x: x[:, 1:, :].reshape(-1, 14, 14, x.shape[2]).permute(0, 3, 1, 2)
    
    def get_cam_weights(self, input_tensor, target_layer, 
                        targets, activations, grads):
        return grads.mean(axis=(2, 3))

# 修改目标层和CAM初始化
target_layers = [model.blocks[-1].attn]  # 使用注意力层代替norm层

# 初始化自定义GradCAM
cam = GetGradCAM(model=model, target_layers=target_layers)
# 读取并预处理图像
rgb_img = cv2.imread(image_path, 1)[:, :, ::-1]
rgb_img = cv2.resize(rgb_img, (224, 224))
rgb_img = np.float32(rgb_img) / 255
input_tensor = preprocess_image(rgb_img, mean=[0.5, 0.5, 0.5], 
                                std=[0.5, 0.5, 0.5]).to(device)

# 执行GradCAM
grayscale_cam = cam(input_tensor=input_tensor, 
                    aug_smooth=aug_smooth, eigen_smooth=eigen_smooth)
# 只选择批次中的第一张图
grayscale_cam = grayscale_cam[0, :]

# 将GradCAM结果叠加到原图上
cam_image = show_cam_on_image(rgb_img, grayscale_cam)

# 保存结果
cv2.imwrite('result/result7/gradcam_vit.jpg', cam_image)
    </code></pre>

    

    <h2>8.py</h2>
    <div class="image-grid-item">
        <img src="/CV_Class/labs/lab3/pic/result8/attn.jpg">
    </div>
    <pre><code class="language-python">
import torch
from PIL import Image
from torchvision import transforms
import numpy as np
import cv2
from deit import load_deit_tiny_from_local
import os

if not os.path.exists("result/result8"):
    os.makedirs("result/result8")
# 定义注意力图的生成函数
def rollout(attentions, discard_ratio, head_fusion):
    # 初始化结果矩阵，获取序列长度（包括CLS token）
    seq_length = attentions[0].size(-1)  # 应该为197 (196 patches + 1 CLS)
    result = torch.eye(seq_length)
    
    with torch.no_grad():
        for attention in attentions:
            # 压缩批次维度并处理多头注意力
            attention = attention.squeeze(0)  # 移除批次维度 [num_heads, seq_len, seq_len]
            
            # 融合多头注意力
            if head_fusion == "mean":
                attention_heads_fused = attention.mean(axis=0)  # 在头维度取平均
            elif head_fusion == "max":
                attention_heads_fused = attention.max(axis=0)[0]
            elif head_fusion == "min":
                attention_heads_fused = attention.min(axis=0)[0]

            # 保持原有处理流程
            flat = attention_heads_fused.view(-1)
            threshold = torch.quantile(flat, discard_ratio)
            flat[flat < threshold] = 0
            attention_heads_fused = flat.view(attention_heads_fused.shape)

            I = torch.eye(attention_heads_fused.size(-1))
            attention_heads_fused += I
            attention_heads_fused = attention_heads_fused /
            attention_heads_fused.sum(dim=-1, keepdim=True)

            # 更新结果矩阵
            result = torch.matmul(attention_heads_fused, result)

    # 修正重塑逻辑
    mask = result[0, 1:]  # 应该得到196个元素 (14x14)
    mask = mask.reshape(14, 14)  # 明确指定patch尺寸
    return mask.numpy()

# 定义VIT注意力图生成类
class VITAttentionRollout:
    """
    Vision Transformer注意力图生成器类
    用于提取和可视化模型的注意力权重
    """
    def __init__(self, model, attention_layer_name='attn_drop', 
    head_fusion="mean", discard_ratio=0.9):
        # 伪代码第11行：加载模型
        self.model = model
        self.attention_layer_name = attention_layer_name
        
        self.head_fusion = head_fusion
        self.discard_ratio = discard_ratio
        
        # 伪代码第12行：注册钩子
        for name, module in self.model.named_modules():
            if self.attention_layer_name in name:
                module.register_forward_hook(self.get_attention)
        # 为所有注意力层注册前向传播钩子，用于收集注意力权重
        
        self.attentions = []

    def get_attention(self, module, input, output):
        """钩子函数：收集前向传播过程中的注意力权重"""
        self.attentions.append(output.cpu())

    def __call__(self, input_tensor):
        """
        生成输入图像的注意力图
        Args:
            input_tensor: 预处理后的输入图像张量
        Returns:
            注意力热力图
        """
        self.attentions = []
        with torch.no_grad():
            output = self.model(input_tensor)

        # 伪代码第14行：调用Rollout函数
        mask = rollout(torch.stack(self.attentions), 
            self.discard_ratio, self.head_fusion)
        return mask  # 添加返回语句
    
# 将注意力图叠加到原始图像上
def show_mask_on_image(img, mask):
    """
    将注意力热力图叠加到原始图像上
    Args:
        img: 原始图像
        mask: 注意力热力图
    Returns:
        可视化后的图像
    """
    # 伪代码第15行：将注意力图可视化并叠加到原始图像
    img = np.float32(img) / 255
    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)
    heatmap = np.float32(heatmap) / 255
    cam = heatmap + np.float32(img)
    cam = cam / np.max(cam)
    return np.uint8(255 * cam)

# 伪代码第11行：加载预训练模型
# 加载DeiT-tiny模型并设置为评估模式
model = load_deit_tiny_from_local(
    "exp3/weights/deit_tiny_patch16_224-a1311bcf.pth")
weights_path = "exp3/weights/deit_tiny_patch16_224-a1311bcf.pth"
model.load_state_dict(torch.load(weights_path)['model'])
model.eval()
# 禁用注意力层的融合计算以便提取注意力权重
for block in model.blocks:
    block.attn.fused_attn = False

# 预处理输入图像
# 加载并预处理输入图像
image_path = '../lab2/vision/data/flower_data/valid/1/image_06739.jpg'
img = Image.open(image_path).convert('RGB')
# 定义图像预处理流程：调整大小、转换为张量、标准化
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
            std=[0.229, 0.224, 0.225]),
])
input_tensor = transform(img).unsqueeze(0)

# 获取注意力图
# 创建注意力图生成器实例并生成注意力图
attention_rollout = VITAttentionRollout(model, head_fusion="mean")
attention_map = attention_rollout(input_tensor)

# 伪代码第15行：可视化并叠加到原始图像
# 将注意力图调整到原始图像大小并生成最终的可视化结果
np_img = np.array(img)[:, :, ::-1]  # 转换为BGR格式（OpenCV格式）
mask = cv2.resize(attention_map, (np_img.shape[1], np_img.shape[0]))
mask = show_mask_on_image(np_img, mask)
cv2.imwrite("result/result8/attn.jpg", mask)  # 保存结果图像
    </code></pre>
</div>

<footer>
    <p>© Copyright Capital Normal University 2025</p>
</footer>
